{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "205c8195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: [1.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "c = np.array([-2, -1])\n",
    "A = np.array([[1, 8/3], [1, 1], [2, 0], [-1, 0], [0, -1]])\n",
    "b = np.array([4, 2, 3, 0, 0])\n",
    "\n",
    "result = linprog(c, A_ub=A, b_ub=b)\n",
    "\n",
    "print(\"Optimal solution:\", result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a77b827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function linprog in module scipy.optimize._linprog:\n",
      "\n",
      "linprog(c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None, method='highs', callback=None, options=None, x0=None, integrality=None)\n",
      "    Linear programming: minimize a linear objective function subject to linear\n",
      "    equality and inequality constraints.\n",
      "    \n",
      "    Linear programming solves problems of the following form:\n",
      "    \n",
      "    .. math::\n",
      "    \n",
      "        \\min_x \\ & c^T x \\\\\n",
      "        \\mbox{such that} \\ & A_{ub} x \\leq b_{ub},\\\\\n",
      "        & A_{eq} x = b_{eq},\\\\\n",
      "        & l \\leq x \\leq u ,\n",
      "    \n",
      "    where :math:`x` is a vector of decision variables; :math:`c`,\n",
      "    :math:`b_{ub}`, :math:`b_{eq}`, :math:`l`, and :math:`u` are vectors; and\n",
      "    :math:`A_{ub}` and :math:`A_{eq}` are matrices.\n",
      "    \n",
      "    Alternatively, that's:\n",
      "    \n",
      "    minimize::\n",
      "    \n",
      "        c @ x\n",
      "    \n",
      "    such that::\n",
      "    \n",
      "        A_ub @ x <= b_ub\n",
      "        A_eq @ x == b_eq\n",
      "        lb <= x <= ub\n",
      "    \n",
      "    Note that by default ``lb = 0`` and ``ub = None`` unless specified with\n",
      "    ``bounds``.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    c : 1-D array\n",
      "        The coefficients of the linear objective function to be minimized.\n",
      "    A_ub : 2-D array, optional\n",
      "        The inequality constraint matrix. Each row of ``A_ub`` specifies the\n",
      "        coefficients of a linear inequality constraint on ``x``.\n",
      "    b_ub : 1-D array, optional\n",
      "        The inequality constraint vector. Each element represents an\n",
      "        upper bound on the corresponding value of ``A_ub @ x``.\n",
      "    A_eq : 2-D array, optional\n",
      "        The equality constraint matrix. Each row of ``A_eq`` specifies the\n",
      "        coefficients of a linear equality constraint on ``x``.\n",
      "    b_eq : 1-D array, optional\n",
      "        The equality constraint vector. Each element of ``A_eq @ x`` must equal\n",
      "        the corresponding element of ``b_eq``.\n",
      "    bounds : sequence, optional\n",
      "        A sequence of ``(min, max)`` pairs for each element in ``x``, defining\n",
      "        the minimum and maximum values of that decision variable. Use ``None``\n",
      "        to indicate that there is no bound. By default, bounds are\n",
      "        ``(0, None)`` (all decision variables are non-negative).\n",
      "        If a single tuple ``(min, max)`` is provided, then ``min`` and\n",
      "        ``max`` will serve as bounds for all decision variables.\n",
      "    method : str, optional\n",
      "        The algorithm used to solve the standard form problem.\n",
      "        :ref:`'highs' <optimize.linprog-highs>` (default),\n",
      "        :ref:`'highs-ds' <optimize.linprog-highs-ds>`,\n",
      "        :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`,\n",
      "        :ref:`'interior-point' <optimize.linprog-interior-point>` (legacy),\n",
      "        :ref:`'revised simplex' <optimize.linprog-revised_simplex>` (legacy),\n",
      "        and\n",
      "        :ref:`'simplex' <optimize.linprog-simplex>` (legacy) are supported.\n",
      "        The legacy methods are deprecated and will be removed in SciPy 1.11.0.\n",
      "    callback : callable, optional\n",
      "        If a callback function is provided, it will be called at least once per\n",
      "        iteration of the algorithm. The callback function must accept a single\n",
      "        `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "    \n",
      "        x : 1-D array\n",
      "            The current solution vector.\n",
      "        fun : float\n",
      "            The current value of the objective function ``c @ x``.\n",
      "        success : bool\n",
      "            ``True`` when the algorithm has completed successfully.\n",
      "        slack : 1-D array\n",
      "            The (nominally positive) values of the slack,\n",
      "            ``b_ub - A_ub @ x``.\n",
      "        con : 1-D array\n",
      "            The (nominally zero) residuals of the equality constraints,\n",
      "            ``b_eq - A_eq @ x``.\n",
      "        phase : int\n",
      "            The phase of the algorithm being executed.\n",
      "        status : int\n",
      "            An integer representing the status of the algorithm.\n",
      "    \n",
      "            ``0`` : Optimization proceeding nominally.\n",
      "    \n",
      "            ``1`` : Iteration limit reached.\n",
      "    \n",
      "            ``2`` : Problem appears to be infeasible.\n",
      "    \n",
      "            ``3`` : Problem appears to be unbounded.\n",
      "    \n",
      "            ``4`` : Numerical difficulties encountered.\n",
      "    \n",
      "            nit : int\n",
      "                The current iteration number.\n",
      "            message : str\n",
      "                A string descriptor of the algorithm status.\n",
      "    \n",
      "        Callback functions are not currently supported by the HiGHS methods.\n",
      "    \n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. All methods accept the following\n",
      "        options:\n",
      "    \n",
      "        maxiter : int\n",
      "            Maximum number of iterations to perform.\n",
      "            Default: see method-specific documentation.\n",
      "        disp : bool\n",
      "            Set to ``True`` to print convergence messages.\n",
      "            Default: ``False``.\n",
      "        presolve : bool\n",
      "            Set to ``False`` to disable automatic presolve.\n",
      "            Default: ``True``.\n",
      "    \n",
      "        All methods except the HiGHS solvers also accept:\n",
      "    \n",
      "        tol : float\n",
      "            A tolerance which determines when a residual is \"close enough\" to\n",
      "            zero to be considered exactly zero.\n",
      "        autoscale : bool\n",
      "            Set to ``True`` to automatically perform equilibration.\n",
      "            Consider using this option if the numerical values in the\n",
      "            constraints are separated by several orders of magnitude.\n",
      "            Default: ``False``.\n",
      "        rr : bool\n",
      "            Set to ``False`` to disable automatic redundancy removal.\n",
      "            Default: ``True``.\n",
      "        rr_method : string\n",
      "            Method used to identify and remove redundant rows from the\n",
      "            equality constraint matrix after presolve. For problems with\n",
      "            dense input, the available methods for redundancy removal are:\n",
      "    \n",
      "            \"SVD\":\n",
      "                Repeatedly performs singular value decomposition on\n",
      "                the matrix, detecting redundant rows based on nonzeros\n",
      "                in the left singular vectors that correspond with\n",
      "                zero singular values. May be fast when the matrix is\n",
      "                nearly full rank.\n",
      "            \"pivot\":\n",
      "                Uses the algorithm presented in [5]_ to identify\n",
      "                redundant rows.\n",
      "            \"ID\":\n",
      "                Uses a randomized interpolative decomposition.\n",
      "                Identifies columns of the matrix transpose not used in\n",
      "                a full-rank interpolative decomposition of the matrix.\n",
      "            None:\n",
      "                Uses \"svd\" if the matrix is nearly full rank, that is,\n",
      "                the difference between the matrix rank and the number\n",
      "                of rows is less than five. If not, uses \"pivot\". The\n",
      "                behavior of this default is subject to change without\n",
      "                prior notice.\n",
      "    \n",
      "            Default: None.\n",
      "            For problems with sparse input, this option is ignored, and the\n",
      "            pivot-based algorithm presented in [5]_ is used.\n",
      "    \n",
      "        For method-specific options, see\n",
      "        :func:`show_options('linprog') <show_options>`.\n",
      "    \n",
      "    x0 : 1-D array, optional\n",
      "        Guess values of the decision variables, which will be refined by\n",
      "        the optimization algorithm. This argument is currently used only by the\n",
      "        'revised simplex' method, and can only be used if `x0` represents a\n",
      "        basic feasible solution.\n",
      "    \n",
      "    integrality : 1-D array or int, optional\n",
      "        Indicates the type of integrality constraint on each decision variable.\n",
      "    \n",
      "        ``0`` : Continuous variable; no integrality constraint.\n",
      "    \n",
      "        ``1`` : Integer variable; decision variable must be an integer\n",
      "        within `bounds`.\n",
      "    \n",
      "        ``2`` : Semi-continuous variable; decision variable must be within\n",
      "        `bounds` or take value ``0``.\n",
      "    \n",
      "        ``3`` : Semi-integer variable; decision variable must be an integer\n",
      "        within `bounds` or take value ``0``.\n",
      "    \n",
      "        By default, all variables are continuous.\n",
      "    \n",
      "        For mixed integrality constraints, supply an array of shape `c.shape`.\n",
      "        To infer a constraint on each decision variable from shorter inputs,\n",
      "        the argument will be broadcasted to `c.shape` using `np.broadcast_to`.\n",
      "    \n",
      "        This argument is currently used only by the ``'highs'`` method and\n",
      "        ignored otherwise.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : OptimizeResult\n",
      "        A :class:`scipy.optimize.OptimizeResult` consisting of the fields\n",
      "        below. Note that the return types of the fields may depend on whether\n",
      "        the optimization was successful, therefore it is recommended to check\n",
      "        `OptimizeResult.status` before relying on the other fields:\n",
      "    \n",
      "        x : 1-D array\n",
      "            The values of the decision variables that minimizes the\n",
      "            objective function while satisfying the constraints.\n",
      "        fun : float\n",
      "            The optimal value of the objective function ``c @ x``.\n",
      "        slack : 1-D array\n",
      "            The (nominally positive) values of the slack variables,\n",
      "            ``b_ub - A_ub @ x``.\n",
      "        con : 1-D array\n",
      "            The (nominally zero) residuals of the equality constraints,\n",
      "            ``b_eq - A_eq @ x``.\n",
      "        success : bool\n",
      "            ``True`` when the algorithm succeeds in finding an optimal\n",
      "            solution.\n",
      "        status : int\n",
      "            An integer representing the exit status of the algorithm.\n",
      "    \n",
      "            ``0`` : Optimization terminated successfully.\n",
      "    \n",
      "            ``1`` : Iteration limit reached.\n",
      "    \n",
      "            ``2`` : Problem appears to be infeasible.\n",
      "    \n",
      "            ``3`` : Problem appears to be unbounded.\n",
      "    \n",
      "            ``4`` : Numerical difficulties encountered.\n",
      "    \n",
      "        nit : int\n",
      "            The total number of iterations performed in all phases.\n",
      "        message : str\n",
      "            A string descriptor of the exit status of the algorithm.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    show_options : Additional options accepted by the solvers.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter.\n",
      "    \n",
      "    `'highs-ds'` and\n",
      "    `'highs-ipm'` are interfaces to the\n",
      "    HiGHS simplex and interior-point method solvers [13]_, respectively.\n",
      "    `'highs'` (default) chooses between\n",
      "    the two automatically. These are the fastest linear\n",
      "    programming solvers in SciPy, especially for large, sparse problems;\n",
      "    which of these two is faster is problem-dependent.\n",
      "    The other solvers (`'interior-point'`, `'revised simplex'`, and\n",
      "    `'simplex'`) are legacy methods and will be removed in SciPy 1.11.0.\n",
      "    \n",
      "    Method *highs-ds* is a wrapper of the C++ high performance dual\n",
      "    revised simplex implementation (HSOL) [13]_, [14]_. Method *highs-ipm*\n",
      "    is a wrapper of a C++ implementation of an **i**\\ nterior-\\ **p**\\ oint\n",
      "    **m**\\ ethod [13]_; it features a crossover routine, so it is as accurate\n",
      "    as a simplex solver. Method *highs* chooses between the two automatically.\n",
      "    For new code involving `linprog`, we recommend explicitly choosing one of\n",
      "    these three method values.\n",
      "    \n",
      "    .. versionadded:: 1.6.0\n",
      "    \n",
      "    Method *interior-point* uses the primal-dual path following algorithm\n",
      "    as outlined in [4]_. This algorithm supports sparse constraint matrices and\n",
      "    is typically faster than the simplex methods, especially for large, sparse\n",
      "    problems. Note, however, that the solution returned may be slightly less\n",
      "    accurate than those of the simplex methods and will not, in general,\n",
      "    correspond with a vertex of the polytope defined by the constraints.\n",
      "    \n",
      "    .. versionadded:: 1.0.0\n",
      "    \n",
      "    Method *revised simplex* uses the revised simplex method as described in\n",
      "    [9]_, except that a factorization [11]_ of the basis matrix, rather than\n",
      "    its inverse, is efficiently maintained and used to solve the linear systems\n",
      "    at each iteration of the algorithm.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Method *simplex* uses a traditional, full-tableau implementation of\n",
      "    Dantzig's simplex algorithm [1]_, [2]_ (*not* the\n",
      "    Nelder-Mead simplex). This algorithm is included for backwards\n",
      "    compatibility and educational purposes.\n",
      "    \n",
      "    .. versionadded:: 0.15.0\n",
      "    \n",
      "    Before applying *interior-point*, *revised simplex*, or *simplex*,\n",
      "    a presolve procedure based on [8]_ attempts\n",
      "    to identify trivial infeasibilities, trivial unboundedness, and potential\n",
      "    problem simplifications. Specifically, it checks for:\n",
      "    \n",
      "    - rows of zeros in ``A_eq`` or ``A_ub``, representing trivial constraints;\n",
      "    - columns of zeros in ``A_eq`` `and` ``A_ub``, representing unconstrained\n",
      "      variables;\n",
      "    - column singletons in ``A_eq``, representing fixed variables; and\n",
      "    - column singletons in ``A_ub``, representing simple bounds.\n",
      "    \n",
      "    If presolve reveals that the problem is unbounded (e.g. an unconstrained\n",
      "    and unbounded variable has negative cost) or infeasible (e.g., a row of\n",
      "    zeros in ``A_eq`` corresponds with a nonzero in ``b_eq``), the solver\n",
      "    terminates with the appropriate status code. Note that presolve terminates\n",
      "    as soon as any sign of unboundedness is detected; consequently, a problem\n",
      "    may be reported as unbounded when in reality the problem is infeasible\n",
      "    (but infeasibility has not been detected yet). Therefore, if it is\n",
      "    important to know whether the problem is actually infeasible, solve the\n",
      "    problem again with option ``presolve=False``.\n",
      "    \n",
      "    If neither infeasibility nor unboundedness are detected in a single pass\n",
      "    of the presolve, bounds are tightened where possible and fixed\n",
      "    variables are removed from the problem. Then, linearly dependent rows\n",
      "    of the ``A_eq`` matrix are removed, (unless they represent an\n",
      "    infeasibility) to avoid numerical difficulties in the primary solve\n",
      "    routine. Note that rows that are nearly linearly dependent (within a\n",
      "    prescribed tolerance) may also be removed, which can change the optimal\n",
      "    solution in rare cases. If this is a concern, eliminate redundancy from\n",
      "    your problem formulation and run with option ``rr=False`` or\n",
      "    ``presolve=False``.\n",
      "    \n",
      "    Several potential improvements can be made here: additional presolve\n",
      "    checks outlined in [8]_ should be implemented, the presolve routine should\n",
      "    be run multiple times (until no further simplifications can be made), and\n",
      "    more of the efficiency improvements from [5]_ should be implemented in the\n",
      "    redundancy removal routines.\n",
      "    \n",
      "    After presolve, the problem is transformed to standard form by converting\n",
      "    the (tightened) simple bounds to upper bound constraints, introducing\n",
      "    non-negative slack variables for inequality constraints, and expressing\n",
      "    unbounded variables as the difference between two non-negative variables.\n",
      "    Optionally, the problem is automatically scaled via equilibration [12]_.\n",
      "    The selected algorithm solves the standard form problem, and a\n",
      "    postprocessing routine converts the result to a solution to the original\n",
      "    problem.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Dantzig, George B., Linear programming and extensions. Rand\n",
      "           Corporation Research Study Princeton Univ. Press, Princeton, NJ,\n",
      "           1963\n",
      "    .. [2] Hillier, S.H. and Lieberman, G.J. (1995), \"Introduction to\n",
      "           Mathematical Programming\", McGraw-Hill, Chapter 4.\n",
      "    .. [3] Bland, Robert G. New finite pivoting rules for the simplex method.\n",
      "           Mathematics of Operations Research (2), 1977: pp. 103-107.\n",
      "    .. [4] Andersen, Erling D., and Knud D. Andersen. \"The MOSEK interior point\n",
      "           optimizer for linear programming: an implementation of the\n",
      "           homogeneous algorithm.\" High performance optimization. Springer US,\n",
      "           2000. 197-232.\n",
      "    .. [5] Andersen, Erling D. \"Finding all linearly dependent rows in\n",
      "           large-scale linear programming.\" Optimization Methods and Software\n",
      "           6.3 (1995): 219-227.\n",
      "    .. [6] Freund, Robert M. \"Primal-Dual Interior-Point Methods for Linear\n",
      "           Programming based on Newton's Method.\" Unpublished Course Notes,\n",
      "           March 2004. Available 2/25/2017 at\n",
      "           https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec14_int_pt_mthd.pdf\n",
      "    .. [7] Fourer, Robert. \"Solving Linear Programs by Interior-Point Methods.\"\n",
      "           Unpublished Course Notes, August 26, 2005. Available 2/25/2017 at\n",
      "           http://www.4er.org/CourseNotes/Book%20B/B-III.pdf\n",
      "    .. [8] Andersen, Erling D., and Knud D. Andersen. \"Presolving in linear\n",
      "           programming.\" Mathematical Programming 71.2 (1995): 221-245.\n",
      "    .. [9] Bertsimas, Dimitris, and J. Tsitsiklis. \"Introduction to linear\n",
      "           programming.\" Athena Scientific 1 (1997): 997.\n",
      "    .. [10] Andersen, Erling D., et al. Implementation of interior point\n",
      "            methods for large scale linear programming. HEC/Universite de\n",
      "            Geneve, 1996.\n",
      "    .. [11] Bartels, Richard H. \"A stabilization of the simplex method.\"\n",
      "            Journal in  Numerische Mathematik 16.5 (1971): 414-434.\n",
      "    .. [12] Tomlin, J. A. \"On scaling linear programming problems.\"\n",
      "            Mathematical Programming Study 4 (1975): 146-166.\n",
      "    .. [13] Huangfu, Q., Galabova, I., Feldmeier, M., and Hall, J. A. J.\n",
      "            \"HiGHS - high performance software for linear optimization.\"\n",
      "            https://highs.dev/\n",
      "    .. [14] Huangfu, Q. and Hall, J. A. J. \"Parallelizing the dual revised\n",
      "            simplex method.\" Mathematical Programming Computation, 10 (1),\n",
      "            119-142, 2018. DOI: 10.1007/s12532-017-0130-5\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Consider the following problem:\n",
      "    \n",
      "    .. math::\n",
      "    \n",
      "        \\min_{x_0, x_1} \\ -x_0 + 4x_1 & \\\\\n",
      "        \\mbox{such that} \\ -3x_0 + x_1 & \\leq 6,\\\\\n",
      "        -x_0 - 2x_1 & \\geq -4,\\\\\n",
      "        x_1 & \\geq -3.\n",
      "    \n",
      "    The problem is not presented in the form accepted by `linprog`. This is\n",
      "    easily remedied by converting the \"greater than\" inequality\n",
      "    constraint to a \"less than\" inequality constraint by\n",
      "    multiplying both sides by a factor of :math:`-1`. Note also that the last\n",
      "    constraint is really the simple bound :math:`-3 \\leq x_1 \\leq \\infty`.\n",
      "    Finally, since there are no bounds on :math:`x_0`, we must explicitly\n",
      "    specify the bounds :math:`-\\infty \\leq x_0 \\leq \\infty`, as the\n",
      "    default is for variables to be non-negative. After collecting coeffecients\n",
      "    into arrays and tuples, the input for this problem is:\n",
      "    \n",
      "    >>> from scipy.optimize import linprog\n",
      "    >>> c = [-1, 4]\n",
      "    >>> A = [[-3, 1], [1, 2]]\n",
      "    >>> b = [6, 4]\n",
      "    >>> x0_bounds = (None, None)\n",
      "    >>> x1_bounds = (-3, None)\n",
      "    >>> res = linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds])\n",
      "    >>> res.fun\n",
      "    -22.0\n",
      "    >>> res.x\n",
      "    array([10., -3.])\n",
      "    >>> res.message\n",
      "    'Optimization terminated successfully. (HiGHS Status 7: Optimal)'\n",
      "    \n",
      "    The marginals (AKA dual values / shadow prices / Lagrange multipliers)\n",
      "    and residuals (slacks) are also available.\n",
      "    \n",
      "    >>> res.ineqlin\n",
      "      residual: [ 3.900e+01  0.000e+00]\n",
      "     marginals: [-0.000e+00 -1.000e+00]\n",
      "    \n",
      "    For example, because the marginal associated with the second inequality\n",
      "    constraint is -1, we expect the optimal value of the objective function\n",
      "    to decrease by ``eps`` if we add a small amount ``eps`` to the right hand\n",
      "    side of the second inequality constraint:\n",
      "    \n",
      "    >>> eps = 0.05\n",
      "    >>> b[1] += eps\n",
      "    >>> linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds]).fun\n",
      "    -22.05\n",
      "    \n",
      "    Also, because the residual on the first inequality constraint is 39, we\n",
      "    can decrease the right hand side of the first constraint by 39 without\n",
      "    affecting the optimal solution.\n",
      "    \n",
      "    >>> b = [6, 4]  # reset to original values\n",
      "    >>> b[0] -= 39\n",
      "    >>> linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds]).fun\n",
      "    -22.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(linprog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac66f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
